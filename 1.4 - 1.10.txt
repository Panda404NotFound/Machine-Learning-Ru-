

1.4 Нормы и метрики

Нормы и метрики являются фундаментальными инструментами в машинном обучении, так как они позволяют измерять расстояния и различия между векторами, что критически важно для оптимизации и сравнения данных. Евклидова норма, или L2-норма, вычисляет расстояние между точками в многомерном пространстве, представляя их как длины отрезков, которые соединяют точки с началом координат. В машинном обучении она используется для измерения ошибки модели и оценки расстояний между представлениями векторов, что помогает при минимизации функции потерь. Манхэттенская норма (L1-норма) измеряет расстояние по «улицам» — полезна, когда мы работаем с разреженными данными, например, в ситуациях, когда значительное число признаков имеют нулевые значения. Косинусное расстояние, в свою очередь, используется для оценки сходства направлений векторов, что особенно актуально в обработке текстов и NLP задачах, где важно не абсолютное значение, а направление векторов (то есть смысл текста).

1.5 Скалярное и векторное произведение

Скалярное произведение позволяет нам вычислять величину, которая указывает на сходство между векторами. Геометрически это можно интерпретировать как степень «угла» между ними: если два вектора направлены в одну сторону, их скалярное произведение будет большим. В машинном обучении, например, это используется в алгоритмах типа перцептронов и нейронных сетей, где для определения активации мы умножаем веса и входы, и таким образом формируем линейную комбинацию. Векторное произведение, более редкое в приложениях нейросетей, полезно в задачах, связанных с трехмерной геометрией, например, для создания ориентации в пространственных задачах.

1.6 Линейные преобразования

Линейные преобразования лежат в основе множества операций машинного обучения, поскольку они представляют собой математические операции, которые применяются к данным для изменения их формы, направления и масштабов. Преобразования определяются с помощью матриц, которые, например, могут сжимать, расширять, вращать или отражать данные в пространстве. Собственные векторы и собственные значения являются важными свойствами матриц, которые помогают найти направления в данных, которые объясняют наибольшую изменчивость, что является основой для методов, таких как PCA (Principal Component Analysis) и SVD (Singular Value Decomposition). Диагонализация и спектральное разложение позволяют упрощать матричные операции, что делает вычисления более эффективными.

1.7 Специальные типы матриц

Симметричные матрицы, ортогональные и разреженные матрицы играют специфические роли в машинном обучении. Симметричные матрицы часто встречаются в задачах, связанных с ковариацией и корреляцией, что позволяет исследовать связи между различными признаками данных. Ортогональные матрицы сохраняют нормы и скалярные произведения, что полезно в алгоритмах для снижения размерности данных, так как они сохраняют структуру данных. Разреженные матрицы особенно важны при работе с большими наборами данных, где многие значения равны нулю, поскольку позволяют эффективно хранить и обрабатывать такие данные, что критично для больших нейронных сетей.

1.8 SVD разложение

SVD разложение, или сингулярное разложение, является мощным методом для снижения размерности, особенно в обработке изображений и текстов. С его помощью можно выделить наиболее значимые компоненты данных и отбрасывать менее значимые, что делает модель более компактной и быстрой. SVD разложение используется в таких методах, как LSA (Latent Semantic Analysis), где оно помогает находить скрытые паттерны в текстовых данных и уменьшать размерность матриц, сохраняя наиболее важные смысловые связи между словами.

1.9 Матричные дифференциалы

В области нейронных сетей матричные дифференциалы важны для понимания того, как изменяются параметры сети при минимизации функции потерь. Производные матриц позволяют точно рассчитывать, как изменяется значение функции потерь по отношению к весам модели. В процессах обучения, таких как градиентный спуск, это позволяет оптимизировать модель, постепенно минимизируя ошибку. Правила дифференцирования матриц расширяют скалярное дифференцирование, позволяя более комплексно и гибко подходить к оптимизации, что крайне важно для многомерных данных и сложных моделей.

1.10 Тензорный анализ углубленно

Тензорный анализ — это расширение понятий векторов и матриц для работы с многомерными данными, где тензоры представляют многомерные массивы. В глубоких нейронных сетях тензоры используются для представления многомерных данных, таких как изображения или временные последовательности. Тензорные сети, графическое представление вычислений, помогают визуализировать взаимосвязи в данных, а тензорные разложения, такие как разложение Канде-Комплекса и Такера, находят применение в задачах сжатия данных и ускорения вычислений. Свёртки тензоров (например, в конволюционных нейронных сетях) позволяют извлекать важные пространственные признаки, что делает их важнейшими компонентами в архитектуре CNN для обработки изображений.

