# Machine-Learning-Ru-
Machine learning documentation and description to learn more about how it works 

ПЛАН ЛЕКЦИЙ:

 I. Математические основы

 1. Линейная алгебра

 1.1 Векторы и векторные пространства
- Понятие вектора и его свойства
- Базис и размерность векторных пространств
- Линейная зависимость и независимость
- Операции с векторами
  - Сложение и вычитание векторов
  - Умножение вектора на скаляр

 1.2 Матричные операции
- Сложение и умножение матриц
- Транспонирование и обращение матриц
- Ранг матрицы и определитель

 1.3 Тензорные вычисления
- Понятие тензора и его ранги
- Операции с тензорами
- Применение тензоров в нейронных сетях

 1.4 Нормы и метрики
- Евклидова норма
- Манхэттенская норма
- Косинусное расстояние

 1.5 Скалярное и векторное произведение
- Свойства скалярного произведения
- Геометрическая интерпретация
- Приложения в проекциях и ортогонализации

 1.6 Линейные преобразования
- Матрицы преобразования
- Собственные векторы и значения
- Диагонализация матриц
- Спектральное разложение
- Приложения в PCA и SVD

 1.7 Специальные типы матриц
- Симметричные матрицы
  - Свойства и приложения
- Ортогональные матрицы
  - Сохранение норм и скалярных произведений
- Разреженные матрицы
  - Хранение и операции с разреженными матрицами

 1.8 SVD разложение
- Теория и алгоритмы вычисления
- Применение в снижении размерности и обработке изображений

 1.9 Матричные дифференциалы
- Производные от матриц по скалярам и матрицам
- Правила дифференцирования

 1.10 Тензорный анализ углубленно
- Тензорные сети
  - Графическое представление вычислений
  - Применение в глубоких нейронных сетях
- Свёртки тензоров
  - Применение в конволюционных нейронных сетях
  - Свойства свертки
- Тензорные разложения
  - Разложение Канде-Комплекса (CP)
  - Разложение Такера
  - Применение в обработке сигналов и изображений

 2. Теория вероятностей и статистика

 2.1 Базовые концепции
- Случайные величины и их распределения
  - Дискретные и непрерывные распределения
  - Функции распределения и плотности
- Условные вероятности
  - Формула Байеса
  - Независимость и зависимость событий
- Марковские цепи
  - Матрицы переходных вероятностей
  - Стационарные распределения
  - Применение в моделировании последовательностей

 2.2 Многомерные распределения
- Многомерное нормальное распределение
  - Ковариационные матрицы
  - Свойства и характеристики
- Копулы
  - Моделирование зависимостей между переменными
  - Применение в финансовой математике
- Смеси распределений
  - Модель смешанных гауссиан
  - Алгоритм Expectation-Maximization (EM)

 2.3 Статистический вывод
- Оценка параметров
  - Точечные и интервальные оценки
  - Метод максимального правдоподобия
- Проверка гипотез
  - Нулевая и альтернативная гипотезы
  - Типы ошибок и уровень значимости
- Доверительные интервалы
  - Построение интервалов для средних и долей
- Байесовский вывод
  - Априорное и апостериорное распределения
  - Применение в байесовских нейронных сетях

 3. Оптимизация

 3.1 Градиентные методы
- Градиентный спуск
  - Основная идея и математическое обоснование
  - Выбор шага обучения
- Момент и ускоренные методы
  - Метод момента
  - Nesterov Accelerated Gradient (NAG)
- Адаптивные методы
  - Adam: комбинация RMSprop и момента
  - RMSprop: сглаживание квадрата градиента
  - Adagrad: адаптивный шаг обучения на основе истории градиентов

 3.2 Стохастическая оптимизация
- Mini-batch градиентный спуск
  - Баланс между SGD и полным градиентным спуском
  - Влияние размера батча на обучение
- Variance reduction
  - Методы уменьшения дисперсии градиентов
  - Алгоритмы SVRG и SAGA
- Параллельный и распределенный SGD
  - Синхронные и асинхронные алгоритмы
  - Влияние на производительность и сходимость

 3.3 Продвинутые методы
- Second-order методы
  - Метод Ньютона и квазиньютоновские методы
  - Применение и вычислительная сложность
- Natural gradient
  - Геометрический подход к оптимизации
  - Fisher Information Matrix
- Evolutionary strategies
  - Генетические алгоритмы
  - Применение в оптимизации без градиентов


 II. Основы машинного обучения

 1. Типы обучения

 1.1 Контролируемое обучение (Supervised Learning)
- Задачи классификации и регрессии
- Методы и алгоритмы
- Применение в реальных задачах

 1.2 Неконтролируемое обучение (Unsupervised Learning)
- Кластеризация и снижение размерности
- Алгоритмы: K-means, PCA
- Выявление скрытых структур в данных

 1.3 Обучение с подкреплением (Reinforcement Learning)
- Агент и среда
- Функция награды
- Алгоритмы Q-learning, SARSA

 2. Основные концепции

 2.1 Функции потерь
- Mean Squared Error (MSE)
- Cross-Entropy Loss
- Выбор функции потерь под задачу

 2.2 Регуляризация
- L1 и L2 регуляризация
- Dropout
- Предотвращение переобучения

 2.3 Переобучение и недообучение
- Признаки и последствия
- Методы обнаружения
- Стратегии решения

 2.4 Валидация и тестирование
- Тренировочный, валидационный и тестовый наборы данных
- Кросс-валидация
- Оценка обобщающей способности модели

 III. Углубленное изучение архитектур нейросетей

 1. Архитектура трансформеров

 1.1 Механизм внимания (Attention)
- Self-Attention механизм
  - Scaled Dot-Product Attention
  - Математическая формулировка
  - Маскирование для параллельной обработки
- Multi-head attention
  - Разделение внимания на несколько подпространств
  - Улучшение способности модели захватывать разные зависимости
- Relative position encoding
  - Учет относительных позиций токенов
  - Преимущества в моделировании последовательностей
- Cross-attention
  - Взаимодействие между различными последовательностями
  - Применение в Encoder-Decoder моделях
- Sparse attention механизмы
  - Снижение вычислительной сложности
  - Методы, такие как Local Attention, Global Attention

 1.2 Архитектурные компоненты
- Encoder блок
  - Self-attention слой
    - Захват внутренних зависимостей
  - Feed-forward слой
    - Нелинейные преобразования
  - Layer normalization
    - Улучшение стабильности обучения
- Decoder блок
  - Masked self-attention
    - Предотвращение утечки информации о будущих токенах
  - Cross-attention
    - Обучение зависимостям между входом и выходом
  - Position-wise feed-forward
    - Применение нелинейности к каждому позиционному вектору

 1.3 Модификации архитектуры
- Reformer
  - Локально чувствительное хеширование для внимания
  - Уменьшение требований к памяти
- Performer
  - Линейное внимание с помощью случайных функций
  - Масштабируемость для длинных последовательностей
- Longformer
  - Комбинация локального и глобального внимания
  - Эффективная обработка длинных текстов
- BigBird
  - Специфические паттерны внимания
  - Теоретические гарантии экспрессивности

 2. Современные архитектуры

 2.1 BERT и его варианты
- RoBERTa
  - Оптимизация процессов обучения
  - Использование большего объема данных
- DeBERTa
  - Дисентанглинг внимания
  - Улучшенные позиционные кодировки
- ALBERT
  - Факторизация матриц эмбеддингов
  - Параметрическое обменивание слоев

 2.2 GPT семейство
- GPT-2
  - Масштабирование модели
  - Проблемы генерации нежелательного контента
- GPT-3
  - Модель с 175 миллиардами параметров
  - Способности к zero-shot и few-shot обучению
- InstructGPT
  - Обучение на основе инструкций
  - Использование обратной связи от пользователей
- GPT-4
  - Мультимодальные возможности
  - Улучшение в понимании контекста и рассуждениях

 2.3 Гибридные архитектуры
- T5
  - Unified Text-to-Text Transfer Transformer
  - Подход к решению разнообразных NLP задач
- BART
  - Денойзинговый автоэнкодер
  - Комбинация преимуществ автокодирования и автогенерации
- PaLM
  - Модели с параллельным обучением
  - Использование масштабируемых архитектур

 2.4 Специализированные архитектуры
- CLIP
  - Обучение на парных данных изображения и текста
  - Применение в задачах компьютерного зрения и NLP
- DALL-E
  - Генерация изображений по текстовым описаниям
  - Креативные возможности модели
- Stable Diffusion
  - Диффузионные модели для генерации изображений
  - Высокое качество и контроль над процессом генерации

 3. RAG (Retrieval-Augmented Generation)

 3.1 Компоненты RAG
- Retriever
  - Поиск релевантной информации из базы знаний
- Generator
  - Генерация ответа на основе полученной информации
- Knowledge Base
  - Хранение и организация данных

 3.2 Векторные базы данных
- Faiss
  - Быстрый поиск по векторным представлениям
- Pinecone
  - Облачная векторная база данных
- Weaviate
  - Семантическая база данных с поддержкой графов знаний

 3.3 Методы поиска
- Dense Retrieval
  - Поиск по плотным векторным представлениям
- Sparse Retrieval
  - Поиск по разреженным векторным представлениям
- Гибридные подходы
  - Комбинация dense и sparse методов для улучшения результатов

 IV. Работа с данными и обучение моделей

 1. Подготовка данных

 1.1 Сбор данных
- Источники данных
  - Публичные датасеты
  - Веб-скрейпинг
  - Корпоративные данные
- Методы сбора
  - API, парсинг, краудсорсинг
- Законодательные аспекты
  - GDPR и другие регуляции
  - Этика сбора данных

 1.2 Предобработка
- Очистка данных
  - Удаление шума и ошибок
- Токенизация
  - Разбиение текста на токены
- Аугментация данных
  - Создание новых данных на основе существующих
- Нормализация
  - Приведение данных к единому формату

 1.3 Форматирование
- JSON/JSONL форматы
  - Структурирование данных для обучения
- Специальные токены
  - Маркеры начала и конца последовательностей
- Prompt Engineering
  - Конструирование входных данных для моделей

 2. Обучение моделей

 2.1 Предварительное обучение (Pre-training)
- Masked Language Modeling (MLM)
  - Технические детали и реализации
  - Преимущества для понимания контекста
- Causal Language Modeling
  - Моделирование вероятности следующего слова
  - Ограничения и области применения
- Контрастивное обучение
  - Позитивные и негативные пары
  - Применение в многомодальных моделях

 2.2 Fine-tuning
- Parameter-efficient fine-tuning
  - LoRA (Low-Rank Adaptation)
    - Сокращение количества обновляемых параметров
    - Применение в ресурсозатратных моделях
  - Prefix-tuning
    - Обучение только префиксов
    - Экономия памяти и вычислительных ресурсов
  - Prompt-tuning
    - Настройка модели с помощью специальных промптов
    - Гибкость и быстрота адаптации
- Instruction fine-tuning
  - Обучение модели пониманию и выполнению инструкций
  - Создание и использование датасетов с инструкциями
- RLHF (Reinforcement Learning from Human Feedback)
  - Использование человеческой оценки для обучения
  - Балансировка между качеством и безопасностью ответов

 2.3 Специальные техники
- Knowledge distillation
  - Передача знаний от большой модели к маленькой
  - Снижение требований к вычислительным ресурсам
- Multi-task learning
  - Обучение на нескольких связанных задачах
  - Улучшение обобщающей способности модели
- Curriculum learning
  - Постепенное увеличение сложности данных
  - Ускорение обучения и улучшение результатов

 3. Оптимизация производительности

 3.1 Эффективность памяти
- Gradient checkpointing
  - Торговля между временем вычисления и потреблением памяти
  - Реализация в современных фреймворках
- Mixed precision training
  - Использование FP16 и FP32
  - Влияние на скорость и точность
- Model parallelism
  - Разделение модели на несколько устройств
  - Сложности в синхронизации и коммуникации

 3.2 Distributed training
- Data parallelism
  - Копирование модели на разные узлы
  - Синхронизация градиентов
- Pipeline parallelism
  - Параллельная обработка разных частей модели
  - Уменьшение времени простоя устройств
- Zero Redundancy Optimizer (ZeRO)
  - Эффективное использование памяти
  - Распределение оптимизаторов и градиентов

 3.3 Inference оптимизация
- Quantization
  - Снижение разрядности весов и активаций
  - Пост-обучающая и во время обучения квантование
- Pruning
  - Удаление ненужных соединений
  - Статическое и динамическое прунинг
- Knowledge distillation
  - Использование упрощенной модели для вывода
  - Баланс между скоростью и точностью

 4. Оценка качества моделей

 4.1 Метрики
- BLEU, ROUGE, METEOR
  - Метрики для оценки качества текстовой генерации
- Perplexity
  - Измерение непредсказуемости модели
- BERTScore
  - Семантическая оценка сходства текстов

 4.2 Человеческая оценка
- Разметка данных
  - Оценка качества модельных предсказаний экспертами
- A/B тестирование
  - Сравнение разных версий моделей
- Сбор обратной связи
  - Использование пользовательских отзывов для улучшения модели

 V. Инструменты разработки и практическое применение

 1. Фреймворки и библиотеки
- PyTorch
  - Динамические вычислительные графы
- TensorFlow
  - Статические графы и TensorFlow 2.x
- JAX
  - Автоматическое дифференцирование и XLA компиляция
- Transformers (Hugging Face)
  - Предобученные модели и токенизаторы
- DeepSpeed
  - Оптимизация обучения больших моделей
- Accelerate
  - Упрощение распределенного обучения

 2. Инструменты мониторинга
- Weights & Biases
  - Отслеживание экспериментов и метрик
- TensorBoard
  - Визуализация обучения
- MLflow
  - Управление жизненным циклом моделей

 3. Практические проекты
- Создание чат-бота на основе LLM
  - Построение диалоговых систем
  - Управление контекстом и состоянием диалога
  - Интеграция с мессенджерами и веб-интерфейсами
- Разработка системы вопрос-ответ
  - Использование RAG для поиска ответов
  - Обработка сложных и многозначных запросов
- Анализ тональности и классификация текста
  - Определение эмоциональной окраски сообщений
  - Классификация по темам и категориям

 VI. Этические и социальные аспекты

 1. Этика в ИИ
- Проблемы предвзятости и дискриминации
  - Анализ источников предвзятости
  - Методы снижения предвзятости в моделях
- Прозрачность и объяснимость моделей
  - Интерпретируемость нейронных сетей
  - Создание объяснимых ИИ систем
- Соответствие нормативным требованиям
  - GDPR и защита персональных данных
  - Соблюдение отраслевых стандартов

 2. Социальное влияние
- Влияние LLM на общество и индустрии
  - Автоматизация и изменение рынка труда
  - Новые возможности и приложения
- Потенциальные риски и выгоды
  - Злоупотребление технологиями ИИ
  - Меры по обеспечению безопасности и ответственности

 VII. Дополнительные ресурсы и развитие

 1. Литература и курсы
- Книги
  - "Deep Learning" — Ian Goodfellow и др.
  - "Neural Networks and Deep Learning" — Michael Nielsen
- Онлайн-курсы
  - Coursera: Deep Learning Specialization — Andrew Ng
  - edX: Machine Learning with Python
  - Fast.ai: Практические курсы по глубокому обучению

 2. Исследовательские материалы
- arXiv
  - Чтение последних препринтов по ИИ и ML
- Конференции
  - NeurIPS
  - ICML
  - ICLR

 3. Сообщества и форумы
- Stack Overflow
  - Решение технических проблем
- Reddit
  - Обсуждение новостей и трендов
- GitHub
  - Участие в открытых проектах
